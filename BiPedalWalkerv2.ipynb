{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product as possibleIterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BipedalWalkerModel:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"BipedalWalker-v2\")\n",
    "        self.obs = self.env.reset()\n",
    "        possibleTorques = np.array([-1.0, 0.0, 1.0])\n",
    "        self.possibleActions = np.array(list(possibleIterations(possibleTorques, possibleTorques, possibleTorques, possibleTorques)))\n",
    "        print(self.possibleActions.shape)\n",
    "        self.initNetworkGraph()\n",
    "        \n",
    "    def initNetworkGraph(self, learningRate = 0.01):\n",
    "        self.nInputLayer = self.env.observation_space.shape[0]  #24\n",
    "        nHiddenLayer1 = 20\n",
    "        nHiddenLayer2 = 40\n",
    "        nOutputLayer = len(self.possibleActions) #81\n",
    "        initializer = tf.variance_scaling_initializer()\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, self.nInputLayer])\n",
    "        hidden1 = tf.layers.dense(self.X, nHiddenLayer1, activation=tf.nn.selu, kernel_initializer=initializer)\n",
    "        hidden2 = tf.layers.dense(hidden1, nHiddenLayer2, activation=tf.nn.selu, kernel_initializer=initializer)\n",
    "        logits = tf.layers.dense(hidden2, nOutputLayer, kernel_initializer=initializer)\n",
    "        outputs = tf.nn.softmax(logits)\n",
    "        \n",
    "        self.logitIndex = tf.squeeze(tf.multinomial(logits, num_samples=1), axis=-1)\n",
    "        y = tf.one_hot(self.logitIndex, depth=len(self.possibleActions))\n",
    "        crossEntropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "        optimizer = tf.train.AdamOptimizer(learningRate)\n",
    "        \n",
    "        gradientsAndVariables = optimizer.compute_gradients(crossEntropy)\n",
    "        self.gradients = [g for g,v in gradientsAndVariables]        \n",
    "        self.gradientPlaceholders = []\n",
    "        gradientsandVariableFeedDict = []\n",
    "        for grad, variable in gradientsAndVariables:\n",
    "            gradientPlaceholder = tf.placeholder(tf.float32, shape=grad.get_shape())\n",
    "            self.gradientPlaceholders.append(gradientPlaceholder)\n",
    "            gradientsandVariableFeedDict.append((gradientPlaceholder, variable))\n",
    "        self.train = optimizer.apply_gradients(gradientsandVariableFeedDict)\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def trainNetwork(self, Iterations = 1000, killAfterSteps = 1000, batchSize = 10, renderEnv = False):\n",
    "        with tf.Session() as session:\n",
    "            tf.global_variables_initializer().run()\n",
    "            for iteration in range(Iterations):\n",
    "                print(\"\\rIteration: {}/{}\".format(iteration + 1, Iterations), end=\"\")\n",
    "                allRewards = []\n",
    "                allGradients = []\n",
    "                for game in range(batchSize):\n",
    "                    currentRewards = []\n",
    "                    currentGradients = []\n",
    "                    obs = self.env.reset()\n",
    "                    for step in range(killAfterSteps):\n",
    "                        if renderEnv:\n",
    "                            self.env.render()\n",
    "                        actionIndex, gradientsValue = session.run([self.logitIndex, self.gradients], feed_dict={self.X: obs.reshape(1, self.nInputLayer)})\n",
    "                        action = self.possibleActions[actionIndex]\n",
    "                        obs, reward, done, info = self.env.step(action[0])\n",
    "                        currentRewards.append(reward)\n",
    "                        currentGradients.append(gradientsValue)\n",
    "                        if done:\n",
    "                            break\n",
    "                    allRewards.append(currentRewards)\n",
    "                    allGradients.append(currentGradients)\n",
    "                \n",
    "                allRewards = self.processRewards(allRewards, rate=0.95)\n",
    "                feed_dict = {}\n",
    "                for i, gradientPlaceholder in enumerate(self.gradientPlaceholders):\n",
    "                    newGradients = [reward * allGradients[gameIndex][step][i]\n",
    "                                      for gameIndex, rewards in enumerate(allRewards)\n",
    "                                          for step, reward in enumerate(rewards)]\n",
    "                    meanGradients = np.mean(newGradients, axis=0)\n",
    "                    feed_dict[gradientPlaceholder] = meanGradients\n",
    "                session.run(self.train, feed_dict=feed_dict)\n",
    "                if iteration % 10 == 0:\n",
    "                    self.saver.save(session, \"./model.ckpt\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    def propagateFinalRewardBackward(self, allRewards, rate ):\n",
    "        finalRewards = np.zeros(len(allRewards))\n",
    "        cumulativeRewards = 0\n",
    "        for step in reversed(range(len(allRewards))):\n",
    "            cumulativeRewards = allRewards[step] + cumulativeRewards * rate\n",
    "            finalRewards[step] = cumulativeRewards\n",
    "        return finalRewards\n",
    "    \n",
    "    def normalizeRewards(self, allRewards):\n",
    "        flattenedRewards = np.concatenate(allRewards)\n",
    "        rewardMean = flattenedRewards.mean()\n",
    "        rewardStd = flattenedRewards.std()\n",
    "        normalizedRewards =  [(reward - rewardMean)/rewardStd for reward in allRewards]\n",
    "        return normalizedRewards\n",
    "    \n",
    "    def processRewards(self, allRewards, rate = 0.8):\n",
    "        propagatedRewards = [self.propagateFinalRewardBackward(rewards, rate) for rewards in allRewards]\n",
    "        normalizedRewards = self.normalizeRewards(propagatedRewards)\n",
    "        return normalizedRewards\n",
    "    \n",
    "    def run(self, model_path = \"./model.ckpt\", maxSteps = 1000 ):\n",
    "        env = gym.make(\"BipedalWalker-v2\")\n",
    "        with tf.Session() as sess:\n",
    "            self.saver.restore(sess, model_path)\n",
    "            obs = self.env.reset()\n",
    "            for step in range(maxSteps):\n",
    "                self.env.render(mode=\"rgb_array\")\n",
    "                action_index_val = self.logitIndex.eval(feed_dict={self.X: obs.reshape(1, self.nInputLayer)})\n",
    "                action = self.possibleActions[action_index_val]\n",
    "                obs, reward, done, info = self.env.step(action[0])\n",
    "                if done:\n",
    "                    break\n",
    "        self.env.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel = BipedalWalkerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.trainNetwork(renderEnv = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
